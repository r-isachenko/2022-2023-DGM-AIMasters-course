{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyfGcNY4pcXd"
   },
   "source": [
    "# Homework2: Variational inference and VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Theory (5pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGamQ3L4W64G"
   },
   "source": [
    "### Problem 1: Log-derivative trick (1pt)\n",
    "\n",
    "In Lecture 3 we encountered a problem with Monte Carlo estimation during deriving the ELBO gradient at the E-step of EM-algorithm. Our density function $q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})$ depends on the gradient parameters $\\boldsymbol{\\phi}$.\n",
    "\\begin{align*}\n",
    "    \\nabla_{\\boldsymbol{\\phi}} \\mathcal{L} (\\boldsymbol{\\phi}, \\boldsymbol{\\theta}) &= \\nabla_{\\boldsymbol{\\phi}} \\int q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) \\left[\\log p(\\mathbf{x}, \\mathbf{z} | \\boldsymbol{\\theta}) - \\log q(\\mathbf{z}| \\mathbf{x}, \\boldsymbol{\\phi}) \\right] d \\mathbf{z} \\\\\n",
    "    & \\neq  \\int q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) \\nabla_{\\boldsymbol{\\phi}} \\left[\\log p(\\mathbf{x}, \\mathbf{z} | \\boldsymbol{\\theta}) - \\log q(\\mathbf{z}| \\mathbf{x}, \\boldsymbol{\\phi}) \\right] d \\mathbf{z} \\\\\n",
    "\\end{align*}\n",
    "The Reparametrization trick allowed us to pass the gradient inside the integral and get a Monte Carlo estimate. \n",
    "\n",
    "However, there is another way to achive this. It is called **log-derivative trick**:\n",
    "$$\n",
    "    \\nabla_\\xi  \\log q(\\eta| \\xi) = \\frac{\\nabla_\\xi q(\\eta| \\xi)}{q(\\eta| \\xi)}.\n",
    "$$\n",
    "\n",
    "1. Get the Monte Carlo estimate of the gradient using the formula for the derivative of the logarithm.\n",
    "\n",
    "2. The final expression often works significantly worse than the reparametrization trick. Namely, it has a huge variance. Try to describe the intuition why the evaluation has a high variance (you need to think about what order and sign the terms in the expression will have)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7SKI68oY2OF"
   },
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: IWAE theory (1pt)\n",
    "\n",
    "In Lecture 4 we discussed [IWAE](https://arxiv.org/abs/1509.00519) model. This model introduces the improved version of the variational lower bound (ELBO):\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}_K (q, \\boldsymbol{\\theta})  = \\mathbb{E}_{\\mathbf{z}_1, \\dots, \\mathbf{z}_K \\sim q(\\mathbf{z} | \\mathbf{x})} \\log \\left( \\frac{1}{K}\\sum_{k=1}^K\\frac{p(\\mathbf{x}, \\mathbf{z}_k | \\boldsymbol{\\theta})}{q(\\mathbf{z}_k| \\mathbf{x})} \\right) \\rightarrow \\max_{q, \\boldsymbol{\\theta}}.\n",
    "$$\n",
    "\n",
    "Here we had the theorem without proof:\n",
    "\n",
    "1. $\\log p(\\mathbf{x} | \\boldsymbol{\\theta}) \\geq \\mathcal{L}_K (q, \\boldsymbol{\\theta}) \\geq \\mathcal{L}_M (q, \\boldsymbol{\\theta}), \\quad \\text{for } K \\geq M$;\n",
    "2.  $\\log p(\\mathbf{x} | \\boldsymbol{\\theta}) = \\lim_{K \\rightarrow \\infty} \\mathcal{L}_K (q, \\boldsymbol{\\theta})$ if $\\frac{p(\\mathbf{x}, \\mathbf{z} | \\boldsymbol{\\theta})}{q(\\mathbf{z} | \\mathbf{x})}$ is bounded.\n",
    "\n",
    "Now it is time to prove it :)\n",
    "\n",
    "**Hints:**\n",
    "1. First part of the theorem.\n",
    "\n",
    "    (a) Use the following equation inside the logarithm of $\\mathcal{L}_K (q, \\boldsymbol{\\theta})$\n",
    "$$\n",
    "    \\frac{a_1 + \\dots + a_K}{K} = \\mathbb{E}_{k_1, \\dots, k_M} \\frac{a_{k_1} + \\dots + a_{k_M}}{M}, \\quad k_1, \\dots, k_M \\sim U[1, K]\n",
    "$$\n",
    "    (b) Apply Jensen' inequality.\n",
    "3. Second part of the theorem: use the Law of large numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: EM-algorithm for GMM (3pt)\n",
    "\n",
    "**Do not worry:** the task is long-written, but it is very useful for the understanding of the EM-algorithm and it is not very hard :)\n",
    "\n",
    "Recall the Gaussian Mixture Model (GMM) we discussed in Seminar 4: \n",
    "\n",
    "- model parameters $\\boldsymbol{\\theta} = \\{ \\boldsymbol{\\pi}_{1:K}, \\boldsymbol{\\mu}_{1:K}, \\boldsymbol{\\Sigma}_{1:K} \\}$;\n",
    "\n",
    "- prior distribution (note that here it also depends on $\\boldsymbol{\\theta}$) $p(z | \\boldsymbol{\\theta}) = \\text{Categorical}(\\pi_1, \\dots \\pi_K)$;\n",
    "\n",
    "- generative distribution $p(\\mathbf{x} | z, \\boldsymbol{\\theta}) = \\mathcal{N}\\left(\\mathbf{x}|, \\boldsymbol{\\mu}_z, \\boldsymbol{\\Sigma}_z\\right)$.\n",
    "\n",
    "Given samples $\\boldsymbol{X} = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\} \\sim p(\\mathbf{x})$ , $\\mathbf{x}_i \\in \\mathbb{R}^m$ we want to fit GMM model via **MLE**:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}^{*} = \\arg\\max\\limits_{\\boldsymbol{\\theta}} \\log p(\\boldsymbol{X} | \\boldsymbol{\\theta}) = \\arg\\max\\limits_{\\boldsymbol{\\theta}}\\sum\\limits_{i = 1}^{n} \\log \\left(\\sum\\limits_{k = 1}^{K}  \\pi_k \\mathcal{N}\\left(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k\\right)\\right).\n",
    "$$\n",
    "\n",
    "The direct solution of the problem above is hard and we substitute **MLE** optimization with **ELBO** optimization:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}^{\\#} = \\arg\\max\\limits_{q, \\boldsymbol{\\theta}} \\mathcal{L}(q, \\boldsymbol{\\theta}) = \\arg\\max\\limits_{q, \\boldsymbol{\\theta}} \\int\\limits_{Z^n} q(z_1, \\dots, z_n) \\log\\frac{ p(\\mathbf{x}_1, z_1, \\dots, \\mathbf{x}_n, z_n | \\boldsymbol{\\theta}) }{ q(z_1, \\dots, z_n) } \\prod\\limits_{i = 1}^{n} d z_i.\n",
    "$$\n",
    "\n",
    "Since the pairs $(\\mathbf{x}_1, z_1), \\dots ,(\\mathbf{x}_N, z_n)$ are mutually independent, the distributions could be factorized:\n",
    "$$\n",
    "    q(z_1, \\dots, z_n) = \\prod_{i=1}^n q(z_i); \\quad p(\\mathbf{x}_1, z_1, \\dots, \\mathbf{x}_n, z_n | \\boldsymbol{\\theta}) = \\prod_{i=1}^n p(\\mathbf{x}_i, z_i | \\boldsymbol{\\theta}).\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}(q, \\boldsymbol{\\theta}) = \\sum\\limits_{i = 1}^{n} \\int\\limits_{Z} q(z_i) \\log\\frac{ p(\\mathbf{x}_i, z_i | \\boldsymbol{\\theta}) }{ q(z_i) } d z_i.\n",
    "$$\n",
    "\n",
    "In the equations above we treet $q(\\cdot)$ as continuous density function. In our case $q(\\cdot)$ is discrete categorical distribution and all integrals are substituted with the corresponding sums: \n",
    "\n",
    "$$\n",
    "\\int\\limits_{Z} f(z) q(z) dz \\longrightarrow \\sum\\limits_{k = 1}^{K} f(k) q(\\{z = k\\}).\n",
    "$$\n",
    "\n",
    "**ELBO** optimization could be done via EM-algorithm:\n",
    "\n",
    "#### EM-algorithm\n",
    "\n",
    "* **E-step**\n",
    "\n",
    "    $$\n",
    "    q(z_i) = p(z_i | \\mathbf{x}_i, \\boldsymbol{\\theta}^{\\text{prev}}).\n",
    "    $$\n",
    "    \n",
    "    Note, that $q(z_i)$ is a categorical distribution over $K$ components: $q(z_i) = \\text{Categorical}(\\xi_1^i, \\xi_2^i, \\dots \\xi_K^i)$.\n",
    "\n",
    "* **M-step**\n",
    "\n",
    "    $$\n",
    "    \\boldsymbol{\\theta}^{\\text{new}} = \\arg\\max\\limits_{\\boldsymbol{\\theta}} \\sum\\limits_{i = 1}^{n} E_{z_i \\sim q(z_i)} \\log p(\\mathbf{x}_i, z_i | \\boldsymbol{\\theta}) = \\arg\\max\\limits_{\\boldsymbol{\\theta}} \\sum\\limits_{i = 1}^{n} \\sum\\limits_{k = 1}^{K} q(\\{z_i = k\\}) \\log p(\\mathbf{x}_i, \\{z_i = k\\} | \\boldsymbol{\\theta}).\n",
    "    $$\n",
    "    \n",
    "#### E- and M- steps derivations\n",
    "    \n",
    "Recall the derivation of **E-step** from the class:\n",
    "\n",
    "$$\n",
    "q(\\{z_i = k\\}) = \\xi_k^i = p(\\{z = k\\} | \\mathbf{x}_i, \\boldsymbol{\\theta}^{\\text{prev}}) \\overset{\\text{Bayes theorem}}{=} \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_i | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum\\limits_{k' = 1}^{K} \\pi_{k'} \\mathcal{N}(\\mathbf{x}_i | \\boldsymbol{\\mu}_{k'}, \\boldsymbol{\\Sigma}_{k'})}.\n",
    "$$\n",
    "\n",
    "**M-step** is a bit harder. Let's denote\n",
    "\n",
    "$$ \n",
    "\\Phi\\left(\\boldsymbol{\\pi}_{1:K}, \\boldsymbol{\\mu}_{1:K} , \\boldsymbol{\\Sigma}_{1:K}\\right) := \\sum\\limits_{i = 1}^{n} \\sum\\limits_{k = 1}^{K} q(\\{z_i = k\\}) \\log p(\\mathbf{x}_i, \\{z_i = k\\} | \\boldsymbol{\\theta}) =\\\\= \\sum\\limits_{i = 1}^{n} \\sum\\limits_{k = 1}^{K} \\xi_k^i \\left(\\log \\pi_k - \\frac{m}{2} \\log 2 \\pi - \\frac{1}{2} \\log \\det \\boldsymbol{\\Sigma}_k - \\frac{1}{2} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k)\\right).\n",
    "$$\n",
    "\n",
    "Let's maximize $\\Phi(\\cdot)$ with respect to parameters $\\boldsymbol{\\pi}_{1:K}$, $\\boldsymbol{\\mu}_{1:K}$, $\\boldsymbol{\\Sigma}_{1:K}$ separately:\n",
    "\n",
    "1. Maximization with respect to $\\boldsymbol{\\pi}_{1:K}$:\n",
    "$$\n",
    "\\arg\\max\\limits_{\\boldsymbol{\\pi}_{1:K}; \\, \\pi_k > 0 \\\\ \\sum\\limits_{k = 1}^{K} \\pi_k = 1} \\Phi\\left(\\boldsymbol{\\pi}_{1:K}, \\boldsymbol{\\mu}_{1:K} , \\boldsymbol{\\Sigma}_{1:K}\\right) = \\arg\\max\\limits_{\\boldsymbol{\\pi}_{1:K}; \\, \\pi_k > 0 \\\\ \\sum\\limits_{k = 1}^{K} \\pi_k = 1} \\sum\\limits_{i = 1}^{n} \\sum\\limits_{k = 1}^{K} \\xi_k^i \\log \\pi_k = \\arg\\max\\limits_{\\boldsymbol{\\pi}_{1:K}; \\, \\pi_k > 0 \\\\ \\sum\\limits_{k = 1}^{K} \\pi_k = 1} \\sum\\limits_{k = 1}^{K} \\log \\left(\\pi_k \\right) \\sum\\limits_{i = 1}^{n} \\xi_k^i.\n",
    "$$\n",
    "\n",
    "    **Subproblem 3.1 (1.5pt):** Prove, that the $\\arg\\max$ problem above has solution $\\pi_k^{\\text{new}} = \\frac{\\sum\\limits_{i = 1}^{n} \\xi_k^i}{n}$, $k \\in 1, 2, \\dots K$.\n",
    "    \n",
    "2. Maximization with respect to $\\boldsymbol{\\mu}_{1:K}$. Let's take the derivative:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\boldsymbol{\\mu}_k} \\Phi\\left(\\boldsymbol{\\pi}_{1:K}, \\boldsymbol{\\mu}_{1:K} , \\boldsymbol{\\Sigma}_{1:K}\\right) = \\frac{\\partial}{\\partial \\boldsymbol{\\mu}_k} \\sum\\limits_{i = 1}^{n} - \\frac{\\xi_{k}^{i}}{2} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k) = \\sum\\limits_{i = 1}^{n} \\left(\\boldsymbol{\\Sigma}_k^{-1} \\mathbf{x}_i - \\boldsymbol{\\Sigma}_k^{-1} \\boldsymbol{\\mu}_k\\right) \\xi_k^i = \\boldsymbol{\\Sigma}_k^{-1} \\sum\\limits_{i = 1}^{n} \\left(\\mathbf{x}_i - \\boldsymbol{\\mu}_k\\right) \\xi_k^i = 0.\n",
    "$$\n",
    "\n",
    "    Since $\\boldsymbol{\\Sigma}_k^{-1}$ is positive definite, the equation above could be written as follows:\n",
    "    \n",
    "    $$\n",
    "    \\sum\\limits_{i = 1}^{n} \\left(\\mathbf{x}_i - \\boldsymbol{\\mu}_k\\right) \\xi_k^i = 0 \\quad \\Rightarrow \\quad \\boldsymbol{\\mu}_k^{\\text{new}} = \\frac{\\sum\\limits_{i = 1}^{n} \\mathbf{x}_i \\xi_k^i}{\\sum\\limits_{i = 1}^{n} \\xi_k^i}.\n",
    "    $$\n",
    "    \n",
    "3. Maximization with respect to $\\boldsymbol{\\Sigma}_{1:K}$.\n",
    "\n",
    "    **Subproblem 3.2 (1.5pt):** Prove, that  \n",
    "    $$\n",
    "    \\boldsymbol{\\Sigma}^{\\text{new}}_k = \\frac{1}{\\sum\\limits_{i = 1}^{n} \\xi_k^i} \\sum\\limits_{i = 1}^{n} \\xi_{k}^{i} \\left(\\mathbf{x}_i - \\boldsymbol{\\mu}^{\\text{new}}_k\\right) \\left(\\mathbf{x}_i - \\boldsymbol{\\mu}^{\\text{new}}_k\\right)^T.\n",
    "    $$\n",
    "\n",
    "    *Hint 1*: $\\frac{\\partial}{\\partial \\mathbf{A}} \\det \\mathbf{A} = \\left( \\det \\mathbf{A} \\right) (\\mathbf{A}^{-1})^T$.\n",
    "    \n",
    "    *Hint 2*: $\\frac{\\partial}{\\partial \\mathbf{A}} \\mathbf{x}^T \\mathbf{A}^{-1} \\mathbf{y} = - (\\mathbf{A}^{-1})^T \\mathbf{x} \\mathbf{y}^T (\\mathbf{A}^{-1})^T$.\n",
    "    \n",
    "    *General Hint*: there is a nice book that helps in working with matrices [matrixcookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf).\n",
    "    \n",
    "    **Subproblem 3.3\\* (extra 1pt):** Prove, that $\\frac{\\partial}{\\partial \\mathbf{A}} \\det \\mathbf{A} = \\left( \\det \\mathbf{A} \\right) (\\mathbf{A}^{-1})^T$.\n",
    "\n",
    "    *Hint*: Recall the notion of *cofactor* or *adjunct* (алгебраическое дополнение) ([wiki_en](https://en.wikipedia.org/wiki/Minor_(linear_algebra)), [wiki_ru](https://ru.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%B5%D0%B1%D1%80%D0%B0%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%B4%D0%BE%D0%BF%D0%BE%D0%BB%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)) from Linear Algebra courses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9leRbWNBqZM2",
    "outputId": "bc3cf49f-897f-49e3-b06b-ace75dda7742"
   },
   "outputs": [],
   "source": [
    "REPO_NAME = \"2022-2023-DGM-AIMasters-course\"\n",
    "!if [ -d {REPO_NAME} ]; then rm -Rf {REPO_NAME}; fi\n",
    "!git clone https://github.com/r-isachenko/{REPO_NAME}.git\n",
    "!cd {REPO_NAME}\n",
    "!pip install ./{REPO_NAME}/homeworks/\n",
    "!rm -Rf {REPO_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nc5RAWFOqekr"
   },
   "outputs": [],
   "source": [
    "from dgm_utils import train_model, plot_training_curves\n",
    "from dgm_utils import visualize_2d_data, visualize_2d_samples\n",
    "from dgm_utils import show_samples, visualize_images, load_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIBqEphlrEGd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0wq164ymr-68",
    "outputId": "285692f9-f6cb-48cc-ed06-06af22ab05a2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZjBDpnudlJL"
   },
   "source": [
    "## Task 2: VAE on 2d data (4pt)\n",
    "\n",
    "In this task we will implement simple VAE model for 2d gaussian distribution $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n",
    "\n",
    "We will consider two cases: \n",
    "* 2d univariate distribution (diagonal covariance matrix $\\boldsymbol{\\Sigma}$);\n",
    "* 2d multivariate distribution (strictly non-diagonal covariance matrix $\\boldsymbol{\\Sigma}$).\n",
    "\n",
    "The goal is to analyze the difference between these two cases and understand why the trained VAE models will behave differently.\n",
    "\n",
    "Below you will find data generation function. Look carefully, do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdTy9DLja_jV"
   },
   "outputs": [],
   "source": [
    "def generate_2d_data(count: int, mode: str = 'univariate') -> Tuple[np.ndarray, np.ndarray]:\n",
    "    assert mode in ['univariate', 'multivariate']\n",
    "    np.random.seed(42)\n",
    "    mean = [[2.0, 3.0]]\n",
    "    sigma = [[3.0, 1.0]]\n",
    "    if mode == 'univariate':\n",
    "        rotate = [\n",
    "            [1.0, 0.0], \n",
    "            [0.0, 1.0]\n",
    "        ]\n",
    "    else:\n",
    "        rotate = [\n",
    "            [np.sqrt(2) / 2, np.sqrt(2) / 2], \n",
    "            [-np.sqrt(2) / 2, np.sqrt(2) / 2]\n",
    "        ]\n",
    "    data = mean + (np.random.randn(count, 2) * sigma).dot(rotate)\n",
    "    data = data.astype('float32')\n",
    "    split = int(0.7 * count)\n",
    "    train_data, test_data = data[:split], data[split:]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ny_F57kMaAc1"
   },
   "source": [
    "Let generate the data and visualize it. We will generate data for two cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "id": "JlZlxRm3a8Mi",
    "outputId": "cd047145-85ad-4c86-c12b-dc1f44c4b216"
   },
   "outputs": [],
   "source": [
    "COUNT = 15000\n",
    "\n",
    "train_data, test_data = generate_2d_data(COUNT, mode='multivariate')\n",
    "visualize_2d_data(train_data, test_data)\n",
    "\n",
    "train_data, test_data = generate_2d_data(COUNT, mode='univariate')\n",
    "visualize_2d_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vRYd_B3f3jz"
   },
   "source": [
    "The difference of these two cases is the form of covariance matrix $\\boldsymbol{\\Sigma}$.\n",
    "\n",
    "In multivariate case the matrix is non-diagonal, in univariate case it is strictly diagonal. As you will see, our VAE model will have absolutely different results for these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biYy9_rWd-DY"
   },
   "source": [
    "Now it is time to define our model. Our model will have the following structure:\n",
    "\n",
    "* The latent dimensionality is equal to 2, the same as the data dimensionality ($\\mathbf{z} \\in \\mathbb{R}^2$, $\\mathbf{x} \\in \\mathbb{R}^2$).\n",
    "* Prior distribution is standard Normal ($p(\\mathbf{z}) = \\mathcal{N}(0, I)$).\n",
    "* Variational posterior distribution (or encoder) is $q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x}))$. Here $\\boldsymbol{\\phi}$ denotes all parameters of the encoder neural network. \n",
    "* Generative distribution (or decoder) is $p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))$. Here $\\boldsymbol{\\theta}$ denotes all parameters of the decoder neural network. Please note, that here we will use continuous distribution for our variables $\\mathbf{x}$.\n",
    "* We will consider only diagonal covariance matrices $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x})$, $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z})$.\n",
    "\n",
    "Model objective is ELBO:\n",
    "$$\n",
    "    \\mathcal{L}(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}) = \\mathbb{E}_{q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})} \\log p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) - KL (q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) || p(\\mathbf{z})).\n",
    "$$\n",
    "\n",
    "To make the expectation is independent of parameters $\\boldsymbol{\\phi}$, we will use reparametrization trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxqxyXZBbKfy"
   },
   "source": [
    "To calculate the loss, we should derive\n",
    "- $\\log p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})$, note that generative distribution is $\\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))$.\n",
    "- KL between $\\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x}))$ and $\\mathcal{N}(0, I)$.\n",
    "\n",
    "Let start with the helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZEUyHuxE39I"
   },
   "outputs": [],
   "source": [
    "def get_normal_KL(mean_1: torch.Tensor, \n",
    "log_std_1: torch.Tensor, \n",
    "mean_2: Optional[torch.Tensor] = None, \n",
    "log_std_2: Optional[torch.Tensor] = None,) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "        This function should return the value of KL(p1 || p2),\n",
    "        where p1 = Normal(mean_1, exp(log_std_1)), p2 = Normal(mean_2, exp(log_std_2) ** 2).\n",
    "        If mean_2 and log_std_2 are None values, we will use standard normal distribution.\n",
    "        Note that we consider the case of diagonal covariance matrix.\n",
    "    \"\"\"\n",
    "    if mean_2 is None:\n",
    "        mean_2 = torch.zeros_like(mean_1)\n",
    "    if log_std_2 is None:\n",
    "        log_std_2 = torch.zeros_like(log_std_1)\n",
    "    # ====\n",
    "    # your code\n",
    "    \n",
    "    # ====\n",
    "\n",
    "\n",
    "def test_KL():\n",
    "    assert np.isclose(get_normal_KL(torch.tensor(2), torch.tensor(3), torch.tensor(0), torch.tensor(0)).numpy(), 200.2144, rtol=1e-3)\n",
    "    assert np.isclose(get_normal_KL(torch.tensor(2), torch.tensor(3), torch.tensor(4), torch.tensor(5)).numpy(), 1.50925, rtol=1e-3)\n",
    "    assert np.allclose(get_normal_KL(torch.tensor((10, 10)), torch.tensor((2, 4)), torch.tensor((3, 5))).numpy(), [49.2990, 1498.479], rtol=1e-3)\n",
    "\n",
    "\n",
    "test_KL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "871Pfpm1TiWF"
   },
   "outputs": [],
   "source": [
    "def get_normal_nll(x: torch.Tensor, mean: torch.Tensor, log_std: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "        This function should return the negative log likelihood log p(x),\n",
    "        where p(x) = Normal(x | mean, exp(log_std) ** 2).\n",
    "        Note that we consider the case of diagonal covariance matrix.\n",
    "    \"\"\"\n",
    "    # ====\n",
    "    # your code\n",
    "    \n",
    "    # ====\n",
    "\n",
    "\n",
    "def test_NLL():\n",
    "    assert np.isclose(get_normal_nll(torch.tensor(2), torch.tensor(2), torch.tensor(3)).numpy(), 3.9189, rtol=1e-3)\n",
    "    assert np.isclose(get_normal_nll(torch.tensor(5), torch.tensor(-3), torch.tensor(6)).numpy(), 6.9191, rtol=1e-3)\n",
    "    assert np.allclose(get_normal_nll(torch.tensor((10, 10)), torch.tensor((2, 4)), torch.tensor((3, 5))).numpy(), np.array([3.9982, 5.9197]), rtol=1e-3)\n",
    "\n",
    "\n",
    "test_NLL()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf2LgTcIbaLV"
   },
   "source": [
    "We will use simple fully connected dense networks for encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KN7NnFplkUSn"
   },
   "outputs": [],
   "source": [
    "class FullyConnectedMLP(nn.Module):\n",
    "    def __init__(self, input_shape: int, hiddens: list, output_shape: int) - > None:\n",
    "        assert isinstance(hiddens, list)\n",
    "        super().__init__()\n",
    "        self.input_shape = (input_shape,)\n",
    "        self.output_shape = (output_shape,)\n",
    "        self.hiddens = hiddens\n",
    "\n",
    "        model = []\n",
    "\n",
    "        # ====\n",
    "        # your code \n",
    "        # stack Dense layers with ReLU activation\n",
    "        # note: you do not have to add relu after the last dense layer\n",
    "        \n",
    "        # ====\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> : torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply network that was defined in __init__ and return the output\n",
    "        \n",
    "        # ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYSbAYdZbyoy"
   },
   "source": [
    "Now it is time to implement the VAE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0r-8le1ubsVr"
   },
   "outputs": [],
   "source": [
    "class VAE2d(nn.Module):\n",
    "    def __init__(self, n_in: int, n_latent: int, enc_hidden_sizes: list, dec_hidden_sizes: list) -> None:\n",
    "        assert isinstance(enc_hidden_sizes, list)\n",
    "        assert isinstance(dec_hidden_sizes, list)\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # define encoder and decoder networks\n",
    "        # the encoder takes n_in elements, has enc_hidden_sizes neurons in hidden layers \n",
    "        # and outputs 2 * n_latent (n_latent for means, and n_latent for std)\n",
    "        # the decoder takes n_latent elements, has dec_hidden_sizes neurons in hidden layers \n",
    "        # and outputs 2 * n_in (n_in for means, and n_in for std)\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def prior(self, n: int) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # return n samples from prior distribution (we use standard normal for prior)\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        # ====\n",
    "        # your code\n",
    "        # now you have to return from the model \n",
    "        # - mu_z - means for variational distribution \n",
    "        # - mu_x - means for generative distribution\n",
    "        # - log_std_z - logarithm of std for variational distribution\n",
    "        # - log_std_x - logarithm of std for generative distribution\n",
    "        # we use logarithm, since the std is always positive\n",
    "        # to get std we will exponentiate it to get rid of this constraint\n",
    "\n",
    "        # 1) mu_z, log_std_z are outputs from the encoder\n",
    "        # 2) apply reparametrization trick to get z (input of decoder)\n",
    "        # (do not forget to use self.prior())\n",
    "        # 3) mu_x, log_std_x are outputs from the decoder\n",
    "        #    Note: [mu, log_std = decoder(input).chunk(2, dim=1)]\n",
    "        \n",
    "        # ====\n",
    "        return mu_z, log_std_z, mu_x, log_std_x\n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> dict:\n",
    "        mu_z, log_std_z, mu_x, log_std_x = self(x)\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply model to get mu_z, log_std_z, mu_x, log_std_x\n",
    "        # 2) compute reconstruction loss using get_normal_nll (it is the first term in ELBO)\n",
    "        # 3) compute KL loss using get_normal_KL (it is the second term in ELBO)\n",
    "        \n",
    "        # ====\n",
    "\n",
    "        return {\n",
    "            'elbo_loss': recon_loss + kl_loss, \n",
    "            'recon_loss': recon_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n",
    "\n",
    "    def sample(self, n: int, sample_from_decoder: bool = True) -> np.ndarray:\n",
    "        with torch.no_grad():\n",
    "            # ====\n",
    "            # your code\n",
    "            # to sample from VAE model you have to sample from prior\n",
    "            # and then apply decoder to prior samples.\n",
    "            # parameter noise indicates whether to sample from decoder\n",
    "            # or just use means of generative distribution as samples\n",
    "            # 1) generate prior samples\n",
    "            # 2) apply decoder\n",
    "            # 3) sample from the decoder distribution if sample_from_decoder=True\n",
    "            \n",
    "            # ====\n",
    "        return z.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjcDe5Qfb8QS"
   },
   "source": [
    "We will use the following function for training our models. Look carefully, do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJglj2srb6Cd"
   },
   "outputs": [],
   "source": [
    "def solve_task(\n",
    "    train_data: np.ndarray, \n",
    "    test_data, model: np.ndarray, \n",
    "    batch_size: int, \n",
    "    epochs: int, \n",
    "    lr: float, \n",
    "    use_cuda: bool = False, \n",
    "    use_tqdm: bool = False\n",
    ") -> None:\n",
    "    train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    train_losses, test_losses = train_model(\n",
    "        model, train_loader, test_loader, epochs=EPOCHS, lr=LR, use_cuda=use_cuda, use_tqdm=use_tqdm, loss_key='elbo_loss'\n",
    "    )\n",
    "    samples_noise = model.sample(3000, sample_from_decoder=True)\n",
    "    samples_nonoise = model.sample(3000, sample_from_decoder=False)\n",
    "\n",
    "    for key, value in test_losses.items():\n",
    "        print('{}: {:.4f}'.format(key, value[-1]))\n",
    "\n",
    "    plot_training_curves(train_losses, test_losses)\n",
    "    visualize_2d_samples(samples_noise, title='Samples with Decoder Noise')\n",
    "    visualize_2d_samples(samples_nonoise, title='Samples without Decoder Noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDAWwEs8eJWV"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters (2 hidden layers could be enough for encoder and decoder)\n",
    "ENC_HIDDEN_SIZES = \n",
    "DEC_HIDDEN_SIZES = \n",
    "BATCH_SIZE =  # any adequate value\n",
    "EPOCHS =      # < 10\n",
    "LR =          # < 1e-2\n",
    "# ====\n",
    "\n",
    "COUNT = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5C_7qoBYc9va"
   },
   "source": [
    "Firstly, we will train the VAE model for multivariate gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IdJaAD6Ls4hL",
    "outputId": "b802e157-93db-4b25-fdf9-ae4d4baaa85c"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = generate_2d_data(COUNT, mode='multivariate')\n",
    "visualize_2d_data(train_data, test_data)\n",
    "\n",
    "model = VAE2d(2, 2, ENC_HIDDEN_SIZES, DEC_HIDDEN_SIZES)\n",
    "solve_task(train_data, test_data, model, BATCH_SIZE, EPOCHS, LR, use_cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7vH4TkZcUH8"
   },
   "source": [
    "To analyze our models we will use the following function. Look carefully, do not change.\n",
    "\n",
    "This function calculates the mean $\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x})$, and covariances $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x})$ of the variational posterior distribution $q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yd_DTgMZ6sx"
   },
   "outputs": [],
   "source": [
    "def get_latent_stats(model: object, test_data: np.ndarray, use_cuda: bool = True, batch_size: int = 3000) -> tuple:\n",
    "    batch = next(iter(data.DataLoader(test_data, batch_size=batch_size, shuffle=True)))\n",
    "    if use_cuda:\n",
    "        batch = batch.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu_z, log_std_z = model(batch)[:2]\n",
    "        \n",
    "    mu_z = mu_z.cpu().numpy()\n",
    "    std_z = log_std_z.exp().cpu().numpy()\n",
    "\n",
    "    return mu_z, std_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FbtO7n9GOsc2",
    "outputId": "07c2659f-fd27-4001-918f-67fa348406be"
   },
   "outputs": [],
   "source": [
    "# just look at these numbers and read the comments after this task\n",
    "mu_z, std_z = get_latent_stats(model, test_data)\n",
    "\n",
    "print('mu_z = ', mu_z.mean(axis=0), '+-', mu_z.std(axis=0))\n",
    "print('std_z = ', std_z.mean(axis=0), '+-', std_z.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wU-EHfogdGhC"
   },
   "source": [
    "Secondly, we will train the VAE model for univariate gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ob-yRJ8xe7Ns",
    "outputId": "becffb30-e85a-4a82-c9ee-e3798c5686db"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = generate_2d_data(COUNT, mode='univariate')\n",
    "visualize_2d_data(train_data, test_data)\n",
    "\n",
    "model = VAE2d(2, 2, ENC_HIDDEN_SIZES, DEC_HIDDEN_SIZES).cuda()\n",
    "solve_task(train_data, test_data, model, BATCH_SIZE, EPOCHS, LR, use_cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KljEJJWdNyx-",
    "outputId": "749debb6-6af7-46de-96a3-2be786ada334"
   },
   "outputs": [],
   "source": [
    "# just look at these numbers and read comments after this task\n",
    "mu_z, std_z = get_latent_stats(model, test_data)\n",
    "\n",
    "print('mu_z = ', mu_z.mean(axis=0), '+-', mu_z.std(axis=0))\n",
    "print('std_z = ', std_z.mean(axis=0), '+-', std_z.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRCmH4X_6tyQ"
   },
   "source": [
    "After training the VAE model on these 2 datasets, have a look at \"Samples without Decoder Noise\" figures. These figures show the means $\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z})$ of the generative distribution $p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})$. In the case of multivariate gaussian, the means are perfectly aligned with the data distribution. \n",
    "Otherwise, you have to see the strange figure in the univariate gaussian case . This happens due to so called **posterior collapse** (we will discuss it at the one of our lectures).\n",
    "\n",
    "To be brief, the reason is the following. Our posterior distribution $p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))$ is a univariate (covariance matrix is diagonal). Thus, the model does not need latent variable since the data distribution is also univariate. In this case VAE ignores latent variable, cause the model fits the distribution without any information from latent space.\n",
    "\n",
    "If the decoder ignores latent variable, the second term in ELBO (KL) could be low (variational posterior distribution, which is given by encoder model, is close to prior distribution for each datapoint). In the training curves you have to see that KL loss behaves differently in these two cases.\n",
    "\n",
    "The mean and std of variational posterior distribution also proves this concept. For the second case you have to see that mean is almost zero and std is almost one.\n",
    "\n",
    "It is a real problem for generative models and we will discuss later how to overcome it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN3th9zXANgx"
   },
   "source": [
    "## Task 3: VAE on CIFAR10 data (4pt)\n",
    "\n",
    "In this task you will implement VAE model for CIFAR10 dataset. \n",
    "\n",
    "Download the data from [here](https://drive.google.com/file/d/1FZcV8Mm91fiXm2jFnB0jvK5ROyHdJFvj/view?usp=sharing) (you could use the cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38FlWXcwAN3g",
    "outputId": "63cbc599-26f7-43c6-d7f9-36b85c75065b"
   },
   "outputs": [],
   "source": [
    "!gdown --id 1FZcV8Mm91fiXm2jFnB0jvK5ROyHdJFvj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "POBb7efUAQhp",
    "outputId": "40871460-63f4-4c36-a4c2-0d33a0129748"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load_pickle('/content/cifar10.pkl', flatten=False, binarize=False)\n",
    "visualize_images(train_data, 'CIFAR10 samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5RpXM_ZATRg"
   },
   "source": [
    "Here the model specification will be almost the same (as in Task 2) with the following differences:\n",
    "* Now our encoder and decoder will be convolutional.\n",
    "* We do not fit the covariance matrix $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z})$ in the generative distribution $p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})$. We assume that it is identical ($\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}) = \\mathbf{I}$). We will use the $\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z})$ means of the generative distribution $p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})$ as model samples.\n",
    "* Model objective is slightly modified ELBO:\n",
    "$$\n",
    "    \\mathcal{L}(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}) = \\mathbb{E}_{q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})} \\log p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) - \\beta * KL (q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) || p(\\mathbf{z})).\n",
    "$$\n",
    "Here we introduce the parameter $\\beta$. It reweights KL term in the total loss. We will discuss the choice of this parameter later in the course. In this exercise you have to play with it, starting with the value $\\beta = 1$ (standard ELBO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ7cIAhkfhif"
   },
   "source": [
    "Let define our convolutional encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmQFoOXgAX4B"
   },
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, input_shape: tuple, n_latent: int) -> None:\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "        # ====\n",
    "        # your code\n",
    "        # we suggest to use the following architecture\n",
    "        # conv2d(32) -> relu -> conv(64) -> relu -> conv(128) -> relu -> conv(256) -> fc(2 * n_latent)\n",
    "        # but we encourage you to create your own architecture\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply convs\n",
    "        # 2) reshape the output to 2d matrix for last fc layer\n",
    "        # 3) apply fc layer\n",
    "        \n",
    "        # ====\n",
    "        return mu, log_std\n",
    "        \n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, n_latent: int, output_shape: tuple) -> None:\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        self.base_size = (128, output_shape[1] // 8, output_shape[2] // 8)\n",
    "        # ====\n",
    "        # your code\n",
    "        # we suggest to use the following architecture\n",
    "        # fc -> conv2dtranspose(128) -> relu -> conv2dtranspose(64) -> relu \n",
    "        # -> conv2dtranspose(32) -> relu -> conv2dtranspose(3)\n",
    "        # but we encourage you to create your own architecture\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply fc layer\n",
    "        # 2) reshape the output to 4d tensor \n",
    "        # 3) apply conv layers\n",
    "        \n",
    "        # ====\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXtOjEvbftM9"
   },
   "source": [
    "Now it is time to implement VAE model for image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3T4RRzUfrdg"
   },
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, input_shape: tuple, n_latent: int, beta: float = 1) -> None:\n",
    "        super().__init__()\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "        self.beta = beta\n",
    "        # ====\n",
    "        # your code\n",
    "        # define encoder with input size input_shape and output dim n_latent\n",
    "        # define decoder with input dim n_latent and output size input_shape\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def prior(self, n: int, use_cuda: bool = True) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # return n samples from prior distribution (we use standart normal for prior)\n",
    "        \n",
    "        # ====\n",
    "        if use_cuda:\n",
    "            z = z.cuda()\n",
    "        return z\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply encoder to get mu_z, log_std_z\n",
    "        # 2) apply reparametrization trick (use self.prior)\n",
    "        # 3) apply decoder to get mu_x (which corresponds to reconstructed x)\n",
    "        \n",
    "        # ====\n",
    "        return mu_z, log_std_z, x_recon\n",
    "        \n",
    "    def loss(self, x: torch.Tensor) -> dict:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) make forward step to get mu_z, log_std_z, x_recon\n",
    "        # 2) calculate recon_loss (use get_normal_nll)\n",
    "        # 3) calcucalte kl_loss (use get_normal_KL)\n",
    "        \n",
    "        # ==== \n",
    "        return {\n",
    "            'elbo_loss': recon_loss + self.beta * kl_loss, \n",
    "            'recon_loss': recon_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n",
    "\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        with torch.no_grad():\n",
    "            # ====\n",
    "            # your code\n",
    "            # 1) generate prior samples\n",
    "            # 2) apply decoder\n",
    "            \n",
    "            # ====\n",
    "            samples = torch.clamp(x_recon, -1, 1)\n",
    "        return samples.cpu().numpy() * 0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373,
     "referenced_widgets": [
      "850b01e13b8f4074b8359a6df5512a30",
      "1fc9db4e7e334bbfaacee10877fe5601",
      "b12dbfa955724f8eaad56b9f6d027440",
      "e1e0ca8ecebc40ada5fcf79d99ce9427",
      "3aa3cc26bcb9454ab8f8c3e72cc4040c",
      "76ebfdf95d134244acf9fe6073025d01",
      "eb62824d89b74338b473d5579f923ec5",
      "3ae62b26f735442aac1d61eefbbd2632",
      "1f34388d6839497b89cd2b988ef8e2d7",
      "575548cd20a24fceb9c43c70a6ee9ee9",
      "80fe74b84bbe472eac18903f17e0a065"
     ]
    },
    "id": "vxxxclgEAX6a",
    "outputId": "27abb0dc-b1fe-45b1-f64e-26d0d8921c83"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "\n",
    "BATCH_SIZE =  # any adequate value\n",
    "EPOCHS =      # < 16\n",
    "LR =          # < 1e-3\n",
    "N_LATENS =    # 128 < _ < 1024\n",
    "BETA =        # 0.1 < _ < 10\n",
    "# ====\n",
    "\n",
    "# we center the data, because it helps the model to fit\n",
    "centered_train_data = train_data * 2 - 1\n",
    "centered_test_data = test_data * 2 - 1\n",
    "\n",
    "train_loader = data.DataLoader(centered_train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(centered_test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = ConvVAE((3, 32, 32), N_LATENS, BETA)\n",
    "\n",
    "train_losses, test_losses = train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    epochs=EPOCHS, \n",
    "    lr=LR, \n",
    "    loss_key='elbo_loss', \n",
    "    use_tqdm=True, \n",
    "    use_cuda=USE_CUDA, \n",
    ")\n",
    "for key, value in test_losses.items():\n",
    "    print('{}: {:.4f}'.format(key, value[-1]))\n",
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlE8JlD-f1B9"
   },
   "source": [
    "Now we could visualize the model outputs.\n",
    "\n",
    "1. We could sample new images from our model (sample latent variable from the prior and apply the decoder).\n",
    "2. We could visualize image reconstructions (apply the encoder and the decoder to the fixed image).\n",
    "3. Visualize interpolations (apply the encoder to two images $\\mathbf{x}_1$ and $\\mathbf{x}_2$ to obtain the latent variables $\\mathbf{z}_1$ and $\\mathbf{z}_2$, apply the decoder to the latent variables $\\mathbf{z}$ lying on the segment between $\\mathbf{z}_1$ and $\\mathbf{z}_2$).\n",
    "\n",
    "**Note:** it is ok, that your samples are blurry. We do not use difficult architectures and do not tune hyperparameters carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3Lpr1ckHAd2v",
    "outputId": "21581405-7b47-4c0e-e97c-b89e259572bf"
   },
   "outputs": [],
   "source": [
    "samples = model.sample(100)\n",
    "\n",
    "x = next(iter(test_loader))[:50]\n",
    "\n",
    "if USE_CUDA:\n",
    "    x = x.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z, _ = model.encoder(x)\n",
    "    x_recon = torch.clamp(model.decoder(z), -1, 1)\n",
    "reconstructions = torch.stack((x, x_recon), dim=1).view(-1, 3, 32, 32) * 0.5 + 0.5\n",
    "reconstructions = reconstructions.cpu().numpy()\n",
    "\n",
    "x = next(iter(test_loader))[:20].cuda()\n",
    "with torch.no_grad():\n",
    "    z, _ = model.encoder(x)\n",
    "    z1, z2 = z.chunk(2, dim=0)\n",
    "    interps = [model.decoder(z1 * (1 - alpha) + z2 * alpha) for alpha in np.linspace(0, 1, 10)]\n",
    "    interps = torch.stack(interps, dim=1).view(-1, 3, 32, 32)\n",
    "    interps = torch.clamp(interps, -1, 1) * 0.5 + 0.5\n",
    "interps = interps.cpu().numpy()\n",
    "\n",
    "show_samples(reconstructions, 'CIFAR10 reconstructions')\n",
    "show_samples(samples, 'CIFAR10 samples')\n",
    "show_samples(interps, 'CIFAR10 interpolation')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw2_solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1f34388d6839497b89cd2b988ef8e2d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fc9db4e7e334bbfaacee10877fe5601": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3aa3cc26bcb9454ab8f8c3e72cc4040c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80fe74b84bbe472eac18903f17e0a065",
      "placeholder": "​",
      "style": "IPY_MODEL_575548cd20a24fceb9c43c70a6ee9ee9",
      "value": " 10/10 [01:46&lt;00:00, 10.61s/it]"
     }
    },
    "3ae62b26f735442aac1d61eefbbd2632": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "575548cd20a24fceb9c43c70a6ee9ee9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76ebfdf95d134244acf9fe6073025d01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "80fe74b84bbe472eac18903f17e0a065": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "850b01e13b8f4074b8359a6df5512a30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b12dbfa955724f8eaad56b9f6d027440",
       "IPY_MODEL_e1e0ca8ecebc40ada5fcf79d99ce9427",
       "IPY_MODEL_3aa3cc26bcb9454ab8f8c3e72cc4040c"
      ],
      "layout": "IPY_MODEL_1fc9db4e7e334bbfaacee10877fe5601"
     }
    },
    "b12dbfa955724f8eaad56b9f6d027440": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb62824d89b74338b473d5579f923ec5",
      "placeholder": "​",
      "style": "IPY_MODEL_76ebfdf95d134244acf9fe6073025d01",
      "value": "100%"
     }
    },
    "e1e0ca8ecebc40ada5fcf79d99ce9427": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f34388d6839497b89cd2b988ef8e2d7",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ae62b26f735442aac1d61eefbbd2632",
      "value": 10
     }
    },
    "eb62824d89b74338b473d5579f923ec5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
